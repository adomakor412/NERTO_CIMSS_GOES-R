{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "large-stamp",
   "metadata": {},
   "source": [
    "# AI model for training (shark fin) artifacts on GOES-R\n",
    "\n",
    "[How to Train an Image Classifier in PyTorch and use it to Perform Basic Inference on Single Images](https://towardsdatascience.com/how-to-train-an-image-classifier-in-pytorch-and-use-it-to-perform-basic-inference-on-single-images-99465a1e9bf5)\n",
    "\n",
    "[TRAINING A CLASSIFIER](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "appreciated-stress",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "blocked-editing",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed = 42\n",
    "MyTrainData = []\n",
    "MyTestData = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "hungarian-header",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.sampler.SubsetRandomSampler object at 0x7fb81364e908>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-078b6dacb714>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_split_train_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'classes'"
     ]
    }
   ],
   "source": [
    "data_dir = '/scratch/adomakor412/mydataset3/'\n",
    "def load_split_train_test(datadir, valid_size = .2):\n",
    "    train_transforms = transforms.Compose([transforms.Resize(224),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       ])\n",
    "    test_transforms = transforms.Compose([transforms.Resize(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      ])\n",
    "    ind_train = np.floor((1-valid_size) * len(datadir))\n",
    "    img_train = datasets.ImageFolder(datadir, transform=train_transforms)\n",
    "    train_shuffle = np.array(img_train.imgs)\n",
    "    np.random.shuffle(train_shuffle)\n",
    "    \n",
    "    ind_test = np.floor((valid_size) * len(datadir))\n",
    "    img_test = datasets.ImageFolder(datadir, transform=test_transforms)\n",
    "    test_shuffle = np.array(img_test.imgs)\n",
    "    np.random.shuffle(test_shuffle)\n",
    "    \n",
    "    \n",
    "    train_data = np.copy(train_shuffle)\n",
    "    test_data = np.copy(test_shuffle)\n",
    "    \n",
    "    for path in train_data:\n",
    "        MyTrainData.append(path[0])\n",
    "    for path in test_data:\n",
    "        MyTestData.append(path[0])\n",
    "    \n",
    "# #     train_data = datasets.ImageFolder(datadir,       \n",
    "# #                     transform=train_transforms)\n",
    "# #     test_data = datasets.ImageFolder(datadir,\n",
    "# #                     transform=test_transforms)\n",
    "    num_train=(len(datasets.ImageFolder(datadir, transform = train_transforms)))\n",
    "    train_idx, test_idx = len(train_data), len(test_data)\n",
    "    test_sampler = SubsetRandomSampler(ind_test)\n",
    "    train_sampler = SubsetRandomSampler(ind_train)\n",
    "#     num_train = len(train_data)\n",
    "#     indices = list(range(num_train))\n",
    "#     split = int(np.floor(valid_size * num_train))\n",
    "#     np.random.shuffle(indices)\n",
    "#     train_idx, test_idx = indices[split:], indices[:split]\n",
    "#     train_sampler = SubsetRandomSampler(train_idx)\n",
    "#     test_sampler = SubsetRandomSampler(test_idx)\n",
    "#     trainloader = torch.utils.data.DataLoader(train_data,\n",
    "#                    sampler=train_sampler, batch_size=64)\n",
    "#     testloader = torch.utils.data.DataLoader(test_data,\n",
    "#                    sampler=test_sampler, batch_size=64)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(img_train, sampler=train_sampler, batch_size=64)\n",
    "    testloader = torch.utils.data.DataLoader(img_test, sampler=test_sampler, batch_size=64)\n",
    "    \n",
    "    return trainloader, testloader\n",
    "trainloader, testloader = load_split_train_test(data_dir, .2)\n",
    "\n",
    "print(trainloader.dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-reset",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MyTrainData)\n",
    "print(MyTestData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sophisticated-scanning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224]) 0\n"
     ]
    }
   ],
   "source": [
    "for data in trainloader.dataset:\n",
    "    print(data[0].shape, data[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fossil-holiday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                                  else \"cpu\")\n",
    "model = models.resnet50(pretrained=True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "applicable-understanding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=10, bias=True)\n",
       "    (4): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "model.fc = nn.Sequential(nn.Linear(2048, 512),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(0.2),\n",
    "                                 nn.Linear(512, 10),\n",
    "                                 nn.LogSoftmax(dim=1))\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.003)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-romantic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 1/100.. Train loss: 2.274.. Test loss: 0.505.. Test accuracy: 0.930\n",
      "2\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 1/100.. Train loss: 1.322.. Test loss: 0.250.. Test accuracy: 0.930\n",
      "3\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 1/100.. Train loss: 0.329.. Test loss: 2.753.. Test accuracy: 0.070\n",
      "4\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 1/100.. Train loss: 3.647.. Test loss: 0.587.. Test accuracy: 0.884\n",
      "5\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 1/100.. Train loss: 0.400.. Test loss: 0.794.. Test accuracy: 0.930\n",
      "6\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 1/100.. Train loss: 0.675.. Test loss: 1.957.. Test accuracy: 0.884\n",
      "7\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 1/100.. Train loss: 0.963.. Test loss: 2.388.. Test accuracy: 0.884\n",
      "8\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 1/100.. Train loss: 3.205.. Test loss: 2.814.. Test accuracy: 0.884\n",
      "9\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 1/100.. Train loss: 3.597.. Test loss: 2.994.. Test accuracy: 0.884\n",
      "10\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 1/100.. Train loss: 3.076.. Test loss: 1.674.. Test accuracy: 0.930\n",
      "11\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 1/100.. Train loss: 2.874.. Test loss: 1.525.. Test accuracy: 0.930\n",
      "12\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 1/100.. Train loss: 2.341.. Test loss: 2.172.. Test accuracy: 0.884\n",
      "13\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 1/100.. Train loss: 1.434.. Test loss: 1.746.. Test accuracy: 0.884\n",
      "14\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 2/100.. Train loss: 1.852.. Test loss: 1.207.. Test accuracy: 0.884\n",
      "15\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 2/100.. Train loss: 1.149.. Test loss: 0.630.. Test accuracy: 0.884\n",
      "16\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 2/100.. Train loss: 0.437.. Test loss: 0.164.. Test accuracy: 0.934\n",
      "17\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 2/100.. Train loss: 0.154.. Test loss: 1.889.. Test accuracy: 0.163\n",
      "18\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 2/100.. Train loss: 1.831.. Test loss: 0.678.. Test accuracy: 0.487\n",
      "19\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 2/100.. Train loss: 0.474.. Test loss: 0.127.. Test accuracy: 0.930\n",
      "20\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 2/100.. Train loss: 0.112.. Test loss: 0.417.. Test accuracy: 0.884\n",
      "21\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 2/100.. Train loss: 0.361.. Test loss: 0.379.. Test accuracy: 0.930\n",
      "22\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 2/100.. Train loss: 0.318.. Test loss: 0.743.. Test accuracy: 0.884\n",
      "23\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 2/100.. Train loss: 0.451.. Test loss: 0.493.. Test accuracy: 0.930\n",
      "24\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 2/100.. Train loss: 0.340.. Test loss: 0.506.. Test accuracy: 0.930\n",
      "25\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 2/100.. Train loss: 0.733.. Test loss: 0.478.. Test accuracy: 0.930\n",
      "26\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 2/100.. Train loss: 0.194.. Test loss: 0.439.. Test accuracy: 0.930\n",
      "27\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 3/100.. Train loss: 0.507.. Test loss: 0.375.. Test accuracy: 0.930\n",
      "28\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 3/100.. Train loss: 0.793.. Test loss: 0.280.. Test accuracy: 0.930\n",
      "29\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 3/100.. Train loss: 0.123.. Test loss: 0.291.. Test accuracy: 0.884\n",
      "30\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 3/100.. Train loss: 0.110.. Test loss: 0.147.. Test accuracy: 0.888\n",
      "31\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 3/100.. Train loss: 0.071.. Test loss: 0.068.. Test accuracy: 0.992\n",
      "32\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 3/100.. Train loss: 0.028.. Test loss: 0.063.. Test accuracy: 0.996\n",
      "33\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 3/100.. Train loss: 0.050.. Test loss: 0.102.. Test accuracy: 1.000\n",
      "34\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 3/100.. Train loss: 0.147.. Test loss: 0.120.. Test accuracy: 1.000\n",
      "35\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 3/100.. Train loss: 0.094.. Test loss: 0.107.. Test accuracy: 1.000\n",
      "36\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 3/100.. Train loss: 0.079.. Test loss: 0.069.. Test accuracy: 1.000\n",
      "37\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 3/100.. Train loss: 0.050.. Test loss: 0.035.. Test accuracy: 0.996\n",
      "38\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 3/100.. Train loss: 0.049.. Test loss: 0.028.. Test accuracy: 0.996\n",
      "39\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 3/100.. Train loss: 0.046.. Test loss: 0.024.. Test accuracy: 0.996\n",
      "40\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 4/100.. Train loss: 0.040.. Test loss: 0.025.. Test accuracy: 0.996\n",
      "41\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 4/100.. Train loss: 0.016.. Test loss: 0.026.. Test accuracy: 0.992\n",
      "42\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 4/100.. Train loss: 0.068.. Test loss: 0.026.. Test accuracy: 0.992\n",
      "43\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 4/100.. Train loss: 0.023.. Test loss: 0.033.. Test accuracy: 0.992\n",
      "44\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 4/100.. Train loss: 0.046.. Test loss: 0.025.. Test accuracy: 0.992\n",
      "45\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 4/100.. Train loss: 0.012.. Test loss: 0.026.. Test accuracy: 0.992\n",
      "46\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 4/100.. Train loss: 0.010.. Test loss: 0.017.. Test accuracy: 0.996\n",
      "47\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 4/100.. Train loss: 0.030.. Test loss: 0.061.. Test accuracy: 0.950\n",
      "48\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 4/100.. Train loss: 0.037.. Test loss: 0.013.. Test accuracy: 0.996\n",
      "49\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 4/100.. Train loss: 0.012.. Test loss: 0.011.. Test accuracy: 1.000\n",
      "50\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 4/100.. Train loss: 0.013.. Test loss: 0.042.. Test accuracy: 1.000\n",
      "51\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 4/100.. Train loss: 0.013.. Test loss: 0.010.. Test accuracy: 1.000\n",
      "52\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 4/100.. Train loss: 0.020.. Test loss: 0.014.. Test accuracy: 1.000\n",
      "53\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 5/100.. Train loss: 0.009.. Test loss: 0.013.. Test accuracy: 1.000\n",
      "54\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 5/100.. Train loss: 0.013.. Test loss: 0.012.. Test accuracy: 0.996\n",
      "55\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 5/100.. Train loss: 0.015.. Test loss: 0.009.. Test accuracy: 0.996\n",
      "56\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 5/100.. Train loss: 0.006.. Test loss: 0.009.. Test accuracy: 0.996\n",
      "57\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 5/100.. Train loss: 0.017.. Test loss: 0.009.. Test accuracy: 0.996\n",
      "58\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 5/100.. Train loss: 0.018.. Test loss: 0.008.. Test accuracy: 1.000\n",
      "59\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 5/100.. Train loss: 0.011.. Test loss: 0.007.. Test accuracy: 1.000\n",
      "60\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 5/100.. Train loss: 0.009.. Test loss: 0.007.. Test accuracy: 1.000\n",
      "61\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n",
      "Epoch 5/100.. Train loss: 0.013.. Test loss: 0.007.. Test accuracy: 1.000\n",
      "62\n",
      "REACHED RUNNING LOSS\n",
      "I am working\n"
     ]
    }
   ],
   "source": [
    "epochs = 100#1\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "print_every = 1#10\n",
    "train_losses, test_losses = [], []\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels in trainloader:\n",
    "        steps += 1\n",
    "        print(steps)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logps = model.forward(inputs)\n",
    "        loss = criterion(logps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        print('REACHED RUNNING LOSS')\n",
    "        if steps % print_every == 0:\n",
    "            print('I am working')\n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in testloader:\n",
    "                    inputs, labels = inputs.to(device),labels.to(device)\n",
    "                    logps = model.forward(inputs)\n",
    "                    batch_loss = criterion(logps, labels)\n",
    "                    test_loss += batch_loss.item()\n",
    "                    \n",
    "                    ps = torch.exp(logps)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "            train_losses.append(running_loss/len(trainloader))\n",
    "            test_losses.append(test_loss/len(testloader))                    \n",
    "            print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                  f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
    "                  f\"Test loss: {test_loss/len(testloader):.3f}.. \"\n",
    "                  f\"Test accuracy: {accuracy/len(testloader):.3f}\")\n",
    "            running_loss = 0\n",
    "            model.train()\n",
    "#torch.save(model, 'aerialmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-harassment",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-holly",
   "metadata": {},
   "source": [
    "#### Took 1 hour to train. Dataset has 100 sharkfins and 889 fillins. Together that is 989 samples. The train to test split is 80% to 20%. Average test accuracy is over 90%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-macro",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
