{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "straight-enlargement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from PIL import Image\n",
    "#import numpy as np\n",
    "import os\n",
    "\n",
    "__author__ = \"Andres Vourakis\"\n",
    "__email__ = \"andresvourakis@gmail.com\"\n",
    "__license__ = \"GPL\"\n",
    "__data__ = \"May 25, 2017\"\n",
    "\n",
    "\n",
    "def image_to_byte_array(image, class_index, size):\n",
    "\n",
    "    img = Image.open(image)\n",
    "    \n",
    "    #Resize\n",
    "    img = img.resize(size) #TODO Check if resizing to given dim can be done\n",
    "\n",
    "    #Convert image to 3 dimensional array\n",
    "    img_array = np.array(img)\n",
    "\n",
    "    #Convert 3 dimensional array into row major order\n",
    "    img_array_R = img_array[:,:,0].flatten()\n",
    "    img_array_G = img_array[:,:,1].flatten()\n",
    "    img_array_B = img_array[:,:,2].flatten()\n",
    "    class_index = [class_index]\n",
    "\n",
    "    # Turn row-major array into bytes\n",
    "    #img_byte_array = np.concatenate((img_array_R, img_array_G, img_array_B)).tobytes() #Turn into row-major byte array\n",
    "    img_byte_array = np.array(list(class_index) + list(img_array_R) + list(img_array_G) + list(img_array_B), np.uint8) #Turn into row-major byte array\n",
    "    \n",
    "    return img_byte_array\n",
    "\n",
    "def create_meta_data(class_labels, destination):\n",
    "    \n",
    "    '''\n",
    "        TODO: Check if directory exists\n",
    "    '''\n",
    "    \n",
    "    file_name = 'batches_meta.txt'\n",
    "    file_path = os.path.join(destination, file_name)\n",
    "    with open(file_path, 'w') as file:\n",
    "        for label in class_labels:\n",
    "            file.write(str(label) + '\\n')\n",
    "\n",
    "\n",
    "def label_to_index(class_labels, class_label):\n",
    "    return class_labels.index(class_label)\n",
    "\n",
    "def open_batch(destination):\n",
    "    file_name = 'data_batch_' + str(CURRENT_BATCH) + '.bin'\n",
    "    file_path = os.path.join(destination, file_name)\n",
    "    return open(file_path, 'wb')\n",
    "\n",
    "def close_batch(file):\n",
    "    file.close()\n",
    "\n",
    "def process_image_dataset(source, destination, size = (32, 32), batch = 1):\n",
    "    \"\"\" \n",
    "        Processes dataset into binary version of CIFAR-10 dataset\n",
    "\n",
    "    Args:\n",
    "        source: Abosulute path to directory containing subdirectories of image datasets.\n",
    "        destination: Absolute path of directory where to save process image datasets.\n",
    "        size (default = (32,32)): square dimensions (width and height) to resize images\n",
    "        batch (default = 1): Number of batches to divide image dataset.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    class_labels = next(os.walk(source))[1]\n",
    "    #dataset_size = len(next(os.walk(source))[2]) #Only gives tot number of files in current directory\n",
    "\n",
    "    dataset_size = 0\n",
    "\n",
    "    for root, subdirs, files in os.walk(source):\n",
    "        dataset_size += len(files) #TODO: Find more efficient way of getting tot num of files\n",
    "    \n",
    "   \n",
    "    batch_size = dataset_size / batch # Total number of images per batch\n",
    "    REACHED_BATCH_MAX = False\n",
    "    CURRENT_BATCH = 1\n",
    "\n",
    "    #create meta data file\n",
    "    create_meta_data(class_labels, destination) #TODO: Check time complex. \n",
    "    batch = open_batch(destination)\n",
    "\n",
    "    #load data and output data\n",
    "    for root, subdirs, files in os.walk(source):\n",
    "\n",
    "        class_label = os.path.relpath(root, source) \n",
    "\n",
    "        if(class_label != '.'): # Ignore source directory\n",
    "\n",
    "            class_index = label_to_index(class_labels, class_label) # class index in bytes\n",
    "             \n",
    "            #data = np.array([image_to_byte_array(os.path.join(root, file), size) for file in files]) #Turn images to numpy array and save into data array\n",
    "            for counter, file in enumerate(files):\n",
    "                \n",
    "                if(REACHED_BATCH_MAX):\n",
    "                    #open new batch and increment CURRENT_BATCH\n",
    "                    CURRENT_BATCH += 1\n",
    "                    batch = open_batch(destination)\n",
    "    \n",
    "                file_path = os.path.join(root, file)\n",
    "                image_byte_array = image_to_byte_array(file_path, class_index, size)\n",
    "                \n",
    "                #write to file while max batch size hasnt been reached!\n",
    "                batch.write(image_byte_array)\n",
    "                \n",
    "                if(counter == batch_size):\n",
    "                    #close current batch and set REACHED_MAX_BATCH to TRUE\n",
    "                    close_batch(batch)\n",
    "                    REACHED_BATCH_MAX = True\n",
    "                        \n",
    "\n",
    "    close_batch(batch) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sustained-special",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CURRENT_BATCH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d59bca78fdb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Process dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mprocess_image_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-d79854505255>\u001b[0m in \u001b[0;36mprocess_image_dataset\u001b[0;34m(source, destination, size, batch)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m#create meta data file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mcreate_meta_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#TODO: Check time complex.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;31m#load data and output data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-d79854505255>\u001b[0m in \u001b[0;36mopen_batch\u001b[0;34m(destination)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mopen_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data_batch_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCURRENT_BATCH\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.bin'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestination\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CURRENT_BATCH' is not defined"
     ]
    }
   ],
   "source": [
    "#from dataset_to_cifar import process_dataset\n",
    "\n",
    "\n",
    "# Set square dimensions of images\n",
    "size = (5424,5424) # 32 by 32 pixels\n",
    "\n",
    "# Set number of batches\n",
    "batch = 1\n",
    "\n",
    "# Source of image dataset (Use absolute path)\n",
    "source = '/home/radomako/train_set_color/sharkfin/'\n",
    "\n",
    "# Destination of processed dataset (use absolute path)\n",
    "destination = 'myData'\n",
    "\n",
    "# Process dataset\n",
    "process_image_dataset(source, destination, size, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-power",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
